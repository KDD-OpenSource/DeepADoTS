{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['prediction_window_size']= 10\n",
    "args['beta'] = 1.0\n",
    "#args['save_fig'] = False\n",
    "#args['compensate'] = True\n",
    "args['data'] = \"XYZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization(seqData,max,min):\n",
    "    return (seqData -min)/(max-min)\n",
    "\n",
    "def standardization(seqData,mean,std):\n",
    "    return (seqData-mean)/std\n",
    "\n",
    "def reconstruct(seqData,mean,std):\n",
    "    return seqData*std+mean\n",
    "\n",
    "\n",
    "def get_accuracy_precision_recall_fscore(ground_truth, prediction):\n",
    "    accuracy = accuracy_score(ground_truth, prediction)\n",
    "    precision, recall, f_score, support = prf(ground_truth, prediction, average='binary')\n",
    "    return accuracy, precision, recall, f_score\n",
    "\n",
    "def get_precision_recall(score, label, num_samples, beta=1.0, sampling='log', predicted_score=None):\n",
    "    '''\n",
    "    :param args:\n",
    "    :param score: anomaly scores\n",
    "    :param label: anomaly labels\n",
    "    :param num_samples: the number of threshold samples\n",
    "    :param beta:\n",
    "    :param scale:\n",
    "    :return:\n",
    "    '''\n",
    "    if predicted_score is not None:\n",
    "        score = score - torch.FloatTensor(predicted_score).squeeze().to(device)\n",
    "\n",
    "    maximum = score.max()\n",
    "    if sampling=='log':\n",
    "        # Sample thresholds logarithmically\n",
    "        # The sampled thresholds are logarithmically spaced between: math:`10 ^ {start}` and: math:`10 ^ {end}`.\n",
    "        th = torch.logspace(0, torch.log10(torch.tensor(maximum)), num_samples)\n",
    "    else:\n",
    "        # Sample thresholds equally\n",
    "        # The sampled thresholds are equally spaced points between: attr:`start` and: attr:`end`\n",
    "        th = torch.linspace(0, maximum, num_samples)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for i in range(len(th)):\n",
    "        anomaly = (score > th[i]).float()\n",
    "        idx = anomaly * 2 + label\n",
    "        tn = (idx == 0.0).sum().item()  # tn\n",
    "        fn = (idx == 1.0).sum().item()  # fn\n",
    "        fp = (idx == 2.0).sum().item()  # fp\n",
    "        tp = (idx == 3.0).sum().item()  # tp\n",
    "\n",
    "        p = tp / (tp + fp + 1e-7)\n",
    "        r = tp / (tp + fn + 1e-7)\n",
    "\n",
    "        if p != 0 and r != 0:\n",
    "            precision.append(p)\n",
    "            recall.append(r)\n",
    "\n",
    "    precision = torch.FloatTensor(precision)\n",
    "    recall = torch.FloatTensor(recall)\n",
    "\n",
    "\n",
    "    f1 = (1 + beta ** 2) * (precision * recall).div(beta ** 2 * precision + recall + 1e-7)\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> calculating precision, recall, and f_beta\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1961be0aff58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# sampling the threshold from 1 to the maximum anomaly score value, either equidistantly or logarithmically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=> calculating precision, recall, and f_beta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m precision, recall, f_beta = get_precision_recall(score, num_samples=1000, beta=beta,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                                  label=TimeseriesData.testLabel.to(device))\n\u001b[1;32m      8\u001b[0m print('data: ', data,' filename: ', filename,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "def fancy_evaluation(X_full, X_test, predicted_scores, prediction_errors):\n",
    "    # For each channel in the dataset\n",
    "    for channel_idx in range(nfeatures):\n",
    "        ''' 4. Evaluate the result '''\n",
    "        # The obtained anomaly scores are evaluated by measuring precision, recall, and f_beta scores\n",
    "        # The precision, recall, f_beta scores are are calculated repeatedly,\n",
    "        # sampling the threshold from 1 to the maximum anomaly score value, either equidistantly or logarithmically.\n",
    "        print('=> calculating precision, recall, and f_beta')\n",
    "        #precision, recall, f_beta = get_precision_recall(args, score, num_samples=1000, beta=args.beta,\n",
    "        #                                                 label=TimeseriesData.testLabel.to(args.device))\n",
    "        accuracy, precision, recall, f_score = get_accuracy_precision_recall_fscore(X_test['y'], predicted_scores)\n",
    "\n",
    "        #print('data: ',args.data,' filename: ',args.filename,\n",
    "        #      ' f-beta (no compensation): ', f_beta.max().item(),' beta: ',args.beta)\n",
    "        #if args['compensate']:\n",
    "        #precision, recall, f_beta = get_precision_recall(args, score, num_samples=1000, beta=args.beta,\n",
    "        #                                                 label=TimeseriesData.testLabel.to(args.device),\n",
    "        #                                                 predicted_score=predicted_score)\n",
    "        #print('data: ',args.data,' filename: ',args.filename,\n",
    "        #      ' f-beta    (compensation): ', f_beta.max().item(),' beta: ',args.beta)\n",
    "\n",
    "        target = reconstruct(test_dataset[channel_idx], X_full[channel_idx].mean(), X_full[channel_idx].std())\n",
    "        mean_prediction = reconstruct(predicted_scores.mean(), X_full[channel_idx].mean(), X_full[channel_idx].std())\n",
    "        oneStep_prediction = reconstruct(predicted_scores[:, -1], X_full[channel_idx].mean(), X_full[channel_idx].std())\n",
    "        Nstep_prediction = reconstruct(predicted_scores[:, 0], X_full[channel_idx].mean(), X_full[channel_idx].std())\n",
    "        \n",
    "        sorted_errors_mean = prediction_errors.abs().mean()\n",
    "        sorted_errors_mean *= TimeseriesData.std[channel_idx]\n",
    "        sorted_errors_mean = sorted_errors_mean\n",
    "        score = score\n",
    "        scores.append(score), targets.append(targets), predicted_scores.append(predicted_score)\n",
    "        mean_predictions.append(mean_prediction), oneStep_predictions.append(oneStep_prediction)\n",
    "        Nstep_predictions.append(Nstep_prediction)\n",
    "        precisions.append(precision), recalls.append(recall), f_betas.append(f_beta)\n",
    "\n",
    "        #if args.save_fig:\n",
    "        #save_dir = Path('result',args.data,args.filename).with_suffix('').joinpath('fig_detection')\n",
    "        #save_dir.mkdir(parents=True,exist_ok=True)\n",
    "        plt.plot(precision, label='precision')\n",
    "        plt.plot(recall, label='recall')\n",
    "        plt.plot(f_beta, label='f1')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Threshold (log scale)')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Anomaly Detection on ' + args.data + ' Dataset', fontsize=18, fontweight='bold')\n",
    "        #plt.savefig(str(save_dir.joinpath('fig_f_beta_channel'+str(channel_idx)).with_suffix('.png')))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(15,5))\n",
    "        ax1.plot(target,label='Target',\n",
    "                 color='black',  marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "        ax1.plot(mean_prediction, label='Mean predictions',\n",
    "                 color='purple', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "        ax1.plot(oneStep_prediction, label='1-step predictions',\n",
    "                 color='green', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "        ax1.plot(Nstep_prediction, label=str(args.prediction_window_size) + '-step predictions',\n",
    "                 color='blue', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "        ax1.plot(sorted_errors_mean,label='Absolute mean prediction errors',\n",
    "                 color='orange', marker='.', linestyle='--', markersize=1, linewidth=1.0)\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1.set_ylabel('Value',fontsize=15)\n",
    "        ax1.set_xlabel('Index',fontsize=15)\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(score.numpy().reshape(-1, 1), label='Anomaly scores from \\nmultivariate normal distribution',\n",
    "                 color='red', marker='.', linestyle='--', markersize=1, linewidth=1)\n",
    "        #if args.compensate:\n",
    "        ax2.plot(predicted_score, label='Predicted anomaly scores from SVR',\n",
    "                 color='cyan', marker='.', linestyle='--', markersize=1, linewidth=1)\n",
    "        #ax2.plot(score.reshape(-1,1)/(predicted_score+1),label='Anomaly scores from \\nmultivariate normal distribution',\n",
    "        #        color='hotpink', marker='.', linestyle='--', markersize=1, linewidth=1)\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.set_ylabel('anomaly score',fontsize=15)\n",
    "        #plt.axvspan(2830,2900 , color='yellow', alpha=0.3)\n",
    "        plt.title('Anomaly Detection on ' + args.data + ' Dataset', fontsize=18, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.xlim([0,len(test_dataset)])\n",
    "        #plt.savefig(str(save_dir.joinpath('fig_scores_channel'+str(channel_idx)).with_suffix('.png')))\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
