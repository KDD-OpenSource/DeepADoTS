{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES IMPORT ------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, Input\n",
    "\n",
    "\n",
    "# function for RMSLE\n",
    "def rmsle(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA DOWNLOAD ========================================================   \n",
    "# air reservation system\n",
    "air_reserve = pd.read_csv('data/air_reserve.csv')\n",
    "air_store_info = pd.read_csv('data/air_store_info.csv')\n",
    "air_visit_data = pd.read_csv('data/air_visit_data.csv')\n",
    "\n",
    "# hpg reservation system\n",
    "hpg_reserve = pd.read_csv('data/hpg_reserve.csv')\n",
    "hpg_store_info = pd.read_csv('data/hpg_store_info.csv')\n",
    "\n",
    "# additional data\n",
    "store_id_relation = pd.read_csv('data/store_id_relation.csv')\n",
    "date_info = pd.read_csv('data/date_info.csv')\n",
    "\n",
    "# test data\n",
    "sample_sub = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION =====================================================\n",
    "# TEST DATA ------------------------------------------------------------\n",
    "# transform test data\n",
    "air_test = sample_sub.copy()\n",
    "air_test['air_store_id'] = air_test['id'].apply(lambda x: str(x)[:-11])\n",
    "air_test['visit_date'] = air_test['id'].apply(lambda x: str(x)[-10:])\n",
    "\n",
    "# dataframe for predictions\n",
    "submission_lstm = air_test.copy()\n",
    "\n",
    "# test set for merger with train set\n",
    "air_test = air_test.drop(['id', 'visitors'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ON RESERVATION --------------------------------------------------\n",
    "# combine air and hpg databases\n",
    "hpg_air_reserve = store_id_relation.join(hpg_reserve.set_index('hpg_store_id'), on = 'hpg_store_id')\n",
    "air_reserve_tmp = air_reserve.copy()\n",
    "hpg_air_reserve = hpg_air_reserve.drop('hpg_store_id', axis = 1)\n",
    "reserve = pd.concat([air_reserve_tmp, hpg_air_reserve])\n",
    "\n",
    "# convert columns of \"reserve\" table into datetime format\n",
    "reserve['visit_datetime'] =  pd.to_datetime(reserve['visit_datetime'])\n",
    "reserve['reserve_datetime'] =  pd.to_datetime(reserve['reserve_datetime'])\n",
    "\n",
    "# create column for visit date inside \"reserve\" table\n",
    "reserve['visit_date'] = reserve['visit_datetime'].apply(lambda x: str(x)[0:10])\n",
    "\n",
    "# calculate the gap between visit time and reservation time inside \"reserve\" table\n",
    "reserve['hour_gap'] = reserve['visit_datetime'].sub(reserve['reserve_datetime'])\n",
    "reserve['hour_gap'] = reserve['hour_gap'].apply(lambda x: x/np.timedelta64(1,'h'))\n",
    "\n",
    "# separate reservation into 5 categories based on gap lenght\n",
    "reserve['reserve_-12_h'] = np.where(reserve['hour_gap'] <= 12,\n",
    "                                    reserve['reserve_visitors'], 0)\n",
    "reserve['reserve_12_37_h'] = np.where((reserve['hour_gap'] <= 37) & (reserve['hour_gap'] > 12),\n",
    "                                       reserve['reserve_visitors'], 0)\n",
    "reserve['reserve_37_59_h'] = np.where((reserve['hour_gap'] <= 59) & (reserve['hour_gap'] > 37),\n",
    "                                       reserve['reserve_visitors'], 0)\n",
    "reserve['reserve_59_85_h'] = np.where((reserve['hour_gap'] <= 85) & (reserve['hour_gap'] > 59),\n",
    "                                       reserve['reserve_visitors'], 0)\n",
    "reserve['reserve_85+_h'] = np.where((reserve['hour_gap'] > 85),\n",
    "                                     reserve['reserve_visitors'], 0)\n",
    "\n",
    "# group by air_store_id and visit_date to enable joining with main table\n",
    "group_list = ['air_store_id', 'visit_date', 'reserve_visitors', 'reserve_-12_h',\n",
    "              'reserve_12_37_h', 'reserve_37_59_h', 'reserve_59_85_h', 'reserve_85+_h']\n",
    "reserve = reserve[group_list].groupby(['air_store_id', 'visit_date'], as_index = False).sum()\n",
    "\n",
    "for i in group_list[2:]:\n",
    "    reserve[i] = reserve[i].apply(lambda x: np.log1p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENRE DATA ----------------------------------------------------------\n",
    "# total amount of restaurants of specific genres by area_name\n",
    "air_genres_area = air_store_info.copy()\n",
    "air_genres_area = air_genres_area[['air_store_id', 'air_genre_name', 'air_area_name']].groupby(['air_genre_name', 'air_area_name'],\n",
    "                                                                                              as_index = False).count()\n",
    "air_genres_area = air_genres_area.rename(columns = {'air_store_id': 'genre_in_area'})\n",
    "\n",
    "# total amount of restaurants in area\n",
    "air_area = air_store_info.copy()\n",
    "air_area = air_area[['air_store_id', 'air_area_name']].groupby(['air_area_name'], as_index = False).count()\n",
    "air_area = air_area.rename(columns = {'air_store_id': 'total_r_in_area'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\HPI\\MP\\venv\\lib\\site-packages\\pandas-0.22.0-py3.5-win-amd64.egg\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# WEEKEND AND HOLIDAYS ------------------------------------------------\n",
    "# additional features for weekends and holidays\n",
    "date_info_mod = date_info.copy()\n",
    "date_info_mod['holiday_eve'] = np.zeros(date_info_mod.shape[0])\n",
    "date_info_mod['holiday_eve'].iloc[:-1] = date_info_mod['holiday_flg'].copy().values[1:]\n",
    "date_info_mod['non_working'] = np.where(date_info_mod['day_of_week'].isin(['Saturday', 'Sunday']) |\n",
    "                                        date_info_mod['holiday_flg'] == 1, 1, 0)\n",
    "date_info_mod = date_info_mod.drop('holiday_flg', axis = 1)\n",
    "\n",
    "# average visitors per restaurant by working and non-working days\n",
    "air_visit_wd = air_visit_data.join(date_info_mod.set_index('calendar_date'), on = 'visit_date')\n",
    "air_visit_wd['visitors'] = air_visit_wd['visitors'].apply(lambda x: np.log1p(x)) \n",
    "\n",
    "# average visitors per restaurant\n",
    "mean_df = air_visit_wd[['visitors',\n",
    "                        'air_store_id',\n",
    "                        'non_working']].copy().groupby(['air_store_id',\n",
    "                                                        'non_working'],\n",
    "                                                        as_index = False).mean()\n",
    "mean_df = mean_df.rename(columns = {'visitors': 'visitors_mean'})\n",
    "\n",
    "\n",
    "# median visitors per restaurant\n",
    "median_df = air_visit_wd[['visitors',\n",
    "                          'air_store_id',\n",
    "                          'non_working']].copy().groupby(['air_store_id',\n",
    "                                                          'non_working'],\n",
    "                                                          as_index = False).median()\n",
    "median_df = median_df.rename(columns = {'visitors': 'visitors_median'})\n",
    "\n",
    "# max visitors per restaurant\n",
    "max_df = air_visit_wd[['visitors',\n",
    "                       'air_store_id',\n",
    "                       'non_working']].copy().groupby(['air_store_id',\n",
    "                                                       'non_working'],\n",
    "                                                       as_index = False).max()\n",
    "max_df = max_df.rename(columns = {'visitors': 'visitors_max'})\n",
    "\n",
    "# min visitors per restaurant\n",
    "min_df = air_visit_wd[['visitors',\n",
    "                       'air_store_id',\n",
    "                       'non_working']].copy().groupby(['air_store_id',\n",
    "                                                       'non_working'],\n",
    "                                                       as_index = False).min()\n",
    "min_df = min_df.rename(columns = {'visitors': 'visitors_min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN TABLES INTO TRAINING AND TEST SETS ----------------------------\n",
    "# function for combining train/test dataset with additional information\n",
    "def merge_join(df):\n",
    "    # add month of visit\n",
    "    df['month'] = df['visit_date'].apply(lambda x: float(str(x)[5:7]))\n",
    "\n",
    "    # add weekday and holiday flag\n",
    "    df = df.join(date_info_mod.set_index('calendar_date'), on = 'visit_date')\n",
    "\n",
    "\n",
    "    # add genre and area name)\n",
    "    df = df.join(air_store_info.set_index('air_store_id'), on = 'air_store_id')\n",
    "\n",
    "    # add quantity of same genre in area\n",
    "    df = pd.merge(df, air_genres_area, how = 'left',\n",
    "                  left_on = ['air_genre_name', 'air_area_name'],\n",
    "                  right_on = ['air_genre_name', 'air_area_name'])\n",
    "\n",
    "\n",
    "    # add total quatity of restaurants in area\n",
    "    df = pd.merge(df, air_area, how = 'left',\n",
    "                  left_on = ['air_area_name'],\n",
    "                  right_on = ['air_area_name'])\n",
    "\n",
    "    # add reservation information\n",
    "    df = pd.merge(df, reserve, how = 'left',\n",
    "                  left_on = ['air_store_id', 'visit_date'],\n",
    "                  right_on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "    # add visitors number mean, median, max and min per each restaurant\n",
    "    df = pd.merge(df, mean_df, how = 'left',\n",
    "                  left_on = ['air_store_id', 'non_working'],\n",
    "                  right_on = ['air_store_id', 'non_working'])\n",
    "\n",
    "    df = pd.merge(df, median_df, how = 'left',\n",
    "                  left_on = ['air_store_id', 'non_working'],\n",
    "                  right_on = ['air_store_id', 'non_working'])\n",
    "    \n",
    "    df = pd.merge(df, max_df, how = 'left',\n",
    "                  left_on = ['air_store_id', 'non_working'],\n",
    "                  right_on = ['air_store_id', 'non_working'])\n",
    "    \n",
    "    df = pd.merge(df, min_df, how = 'left',\n",
    "                  left_on = ['air_store_id', 'non_working'],\n",
    "                  right_on = ['air_store_id', 'non_working'])\n",
    "    \n",
    "    # change NaN to 0\n",
    "    df = df.fillna(0) \n",
    "   \n",
    "    return df\n",
    "\n",
    "# combine train/test data with additional information\n",
    "air_train = air_visit_data.copy()\n",
    "X = merge_join(air_train)\n",
    "X_test = merge_join(air_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE STRING FEATURES ----------------------------------------------\n",
    "# (one-hot encoding may provide better result,\n",
    "# I preferred to apply labels encoding to avoid high dimensional feature space)\n",
    "\n",
    "# Weekday\n",
    "le_weekday = preprocessing.LabelEncoder()\n",
    "le_weekday.fit(X['day_of_week'])\n",
    "X['day_of_week'] = le_weekday.transform(X['day_of_week'])\n",
    "X_test['day_of_week'] = le_weekday.transform(X_test['day_of_week'])\n",
    "\n",
    "# Genre name\n",
    "le_genre = preprocessing.LabelEncoder()\n",
    "le_genre.fit(X['air_genre_name'])\n",
    "X['air_genre_name'] = le_genre.transform(X['air_genre_name'])\n",
    "X_test['air_genre_name'] = le_genre.transform(X_test['air_genre_name'])\n",
    "\n",
    "# Area name\n",
    "le_area = preprocessing.LabelEncoder()\n",
    "le_area.fit(X['air_area_name'])\n",
    "X['air_area_name'] = le_area.transform(X['air_area_name'])\n",
    "X_test['air_area_name'] = le_area.transform(X_test['air_area_name'])\n",
    "\n",
    "# id\n",
    "le_id = preprocessing.LabelEncoder()\n",
    "le_id.fit(X['air_store_id'])\n",
    "X['air_store_id'] = le_id.transform(X['air_store_id'])\n",
    "X_test['air_store_id'] = le_id.transform(X_test['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULTANEOUS TRANSFORMATION OF TRAIN AND TEST SETS -------------------\n",
    "# combine train and test sets\n",
    "X_all = X.append(X_test)\n",
    "\n",
    "# date table (includes all dates for training and test period)\n",
    "dates = np.arange(np.datetime64(X_all.visit_date.min()),\n",
    "                  np.datetime64(X_all.visit_date.max()) + 1,\n",
    "                  datetime.timedelta(days=1))\n",
    "ids = X_all['air_store_id'].unique()\n",
    "dates_all = dates.tolist()*len(ids)\n",
    "ids_all = np.repeat(ids, len(dates.tolist())).tolist()\n",
    "df_all = pd.DataFrame({\"air_store_id\": ids_all, \"visit_date\": dates_all})\n",
    "df_all['visit_date'] = df_all['visit_date'].copy().apply(lambda x: str(x)[:10])\n",
    "\n",
    "# create copy of X_all with data relevant to 'visit_date'\n",
    "X_dates = X_all[['visit_date', 'month', 'day_of_week', 'holiday_eve', 'non_working']].copy()\n",
    "\n",
    "# remove duplicates to avoid memory issues\n",
    "X_dates = X_dates.drop_duplicates('visit_date')\n",
    "\n",
    "# merge dataframe that represents all dates per each restaurant with information about each date\n",
    "df_to_reshape = df_all.merge(X_dates,\n",
    "                             how = \"left\",\n",
    "                             left_on = 'visit_date',\n",
    "                             right_on = 'visit_date')\n",
    "\n",
    "# create copy of X_all with data relevant to 'air_store_id'\n",
    "X_stores = X_all[['air_store_id', 'air_genre_name', 'air_area_name', 'latitude',\n",
    "                  'longitude', 'genre_in_area', 'total_r_in_area']].copy()       \n",
    "\n",
    "# remove duplicates to avoid memory issues\n",
    "X_stores = X_stores.drop_duplicates('air_store_id')\n",
    "\n",
    "# merge dataframe that represents all dates per each restaurant with information about each restaurant\n",
    "df_to_reshape = df_to_reshape.merge(X_stores,\n",
    "                                    how = \"left\",\n",
    "                                    left_on = 'air_store_id',\n",
    "                                    right_on = 'air_store_id')\n",
    "\n",
    "# merge dataframe that represents all dates per each restaurant with inf. about each restaurant per specific date\n",
    "df_to_reshape = df_to_reshape.merge(X_all[['air_store_id', 'visit_date', 'reserve_visitors', 'visitors_mean', \n",
    "                                       'visitors_median', 'visitors_max', 'visitors_min', 'visitors']],\n",
    "                                    how = \"left\",\n",
    "                                    left_on = ['air_store_id', 'visit_date'],\n",
    "                                    right_on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "# separate 'visitors' into output array\n",
    "Y_lstm_df = df_to_reshape[['visit_date', 'air_store_id', 'visitors']].copy().fillna(0)\n",
    "\n",
    "# take log(y+1)\n",
    "Y_lstm_df['visitors'] = np.log1p(Y_lstm_df['visitors'].values)\n",
    "\n",
    "# add flag for days when a restaurant was closed\n",
    "df_to_reshape['closed_flag'] = np.where(df_to_reshape['visitors'].isnull() &\n",
    "                                        df_to_reshape['visit_date'].isin(X['visit_date']).values,1,0)\n",
    "\n",
    "# drop 'visitors' and from dataset\n",
    "df_to_reshape = df_to_reshape.drop(['visitors'], axis = 1)\n",
    "\n",
    "# fill in NaN values\n",
    "df_to_reshape = df_to_reshape.fillna(-1)\n",
    "\n",
    "# list of df_to_reshape columns without 'air_store_id' and 'visit_date'\n",
    "columns_list = [x for x in list(df_to_reshape.iloc[:,2:])]\n",
    "\n",
    "# bound all numerical values between -1 and 1\n",
    "# note: to avoid data leakage 'fit' should be made on traid data and 'transform' on train and test data\n",
    "# in this case all data in test set is taken from train set, thus fit/transform on all data \n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(df_to_reshape[columns_list])\n",
    "df_to_reshape[columns_list] = scaler.transform(df_to_reshape[columns_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFIC PREPARATION FOR NEURAL NETWORK AND ENCODER/DECODER ---------------\n",
    "# reshape X into (samples, timesteps, features)\n",
    "X_all_lstm = df_to_reshape.values[:,2:].reshape(len(ids),\n",
    "                                                len(dates),\n",
    "                                                df_to_reshape.shape[1]-2)\n",
    "\n",
    "# isolate output for train set and reshape it for time series\n",
    "Y_lstm_df = Y_lstm_df.loc[Y_lstm_df['visit_date'].isin(X['visit_date'].values) &\n",
    "                          Y_lstm_df['air_store_id'].isin(X['air_store_id'].values),]\n",
    "Y_lstm = Y_lstm_df.values[:,2].reshape(len(X['air_store_id'].unique()),\n",
    "                                       len(X['visit_date'].unique()),\n",
    "                                       1)\n",
    "\n",
    "# test dates\n",
    "n_test_dates = len(X_test['visit_date'].unique())\n",
    "\n",
    "# make additional features for number of visitors in t-1, t-2, ... t-7\n",
    "t_minus = np.ones([Y_lstm.shape[0],Y_lstm.shape[1],1])\n",
    "for i in range(1,8):\n",
    "    temp = Y_lstm.copy()\n",
    "    temp[:,i:,:] = Y_lstm[:,0:-i,:].copy()\n",
    "    t_minus = np.concatenate((t_minus[...], temp[...]), axis = 2)\n",
    "t_minus = t_minus[:,:,1:]\n",
    "\n",
    "\n",
    "# split X_all into training and test data\n",
    "X_lstm = X_all_lstm[:,:-n_test_dates,:]\n",
    "X_lstm_test = X_all_lstm[:,-n_test_dates:,:]\n",
    "\n",
    "# add t-1, t-2 ... t-7 visitors to feature vector\n",
    "X_lstm = np.concatenate((X_lstm[...], t_minus[...]), axis = 2)\n",
    "\n",
    "# split training set into train and validation sets\n",
    "X_tr = X_lstm[:,39:-140,:]\n",
    "Y_tr = Y_lstm[:,39:-140,:]\n",
    "\n",
    "X_val = X_lstm[:,-140:,:]\n",
    "Y_val = Y_lstm[:,-140:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER-DECODER MODEL ===================================================\n",
    "# many thanks to the following resources:\n",
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "# https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "# http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf\n",
    "# https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/\n",
    "# https://github.com/Arturus/kaggle-web-traffic\n",
    "\n",
    "# MODEL FOR ENCODER AND DECODER -------------------------------------------\n",
    "num_encoder_tokens = X_lstm.shape[2]\n",
    "latent_dim = 64 # to avoid \"kernel run out of time\" situation. I used 256.\n",
    "\n",
    "# encoder training\n",
    "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, \n",
    "               batch_input_shape = (1, None, num_encoder_tokens),\n",
    "               stateful = False,\n",
    "               return_sequences = True,\n",
    "               return_state = True,\n",
    "               recurrent_initializer = 'glorot_uniform')\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c] # 'encoder_outputs' are ignored and only states are kept.\n",
    "\n",
    "# Decoder training, using 'encoder_states' as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "decoder_lstm_1 = LSTM(latent_dim,\n",
    "                      batch_input_shape = (1, None, num_encoder_tokens),\n",
    "                      stateful = False,\n",
    "                      return_sequences = True,\n",
    "                      return_state = False,\n",
    "                      dropout = 0.2,\n",
    "                      recurrent_dropout = 0.2) # True\n",
    "\n",
    "decoder_lstm_2 = LSTM(32, # to avoid \"kernel run out of time\" situation. I used 128.\n",
    "                     stateful = False,\n",
    "                     return_sequences = True,\n",
    "                     return_state = True,\n",
    "                     dropout = 0.2,\n",
    "                     recurrent_dropout = 0.2)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm_2(decoder_lstm_1(decoder_inputs, initial_state = encoder_states))\n",
    "decoder_dense = TimeDistributed(Dense(Y_lstm.shape[2], activation = 'relu'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# training model\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "training_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# useful for understanding the model architecture\n",
    "# training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR APPLIED TO FEED ENCODER AND DECODER ---------------------------\n",
    "# generator that randomly creates times series of 39 consecutive days\n",
    "# theses time series has following 3d shape: 829 restaurants * 39 days * num_features \n",
    "def dec_enc_n_days_gen(X_3d, Y_3d, length):\n",
    "    while 1:\n",
    "        decoder_boundary = X_3d.shape[1] - length - 1\n",
    "        \n",
    "        encoder_start = np.random.randint(0, decoder_boundary)\n",
    "        encoder_end = encoder_start + length\n",
    "        \n",
    "        decoder_start = encoder_start + 1\n",
    "        decoder_end = encoder_end + 1\n",
    "        \n",
    "        X_to_conc = X_3d[:, encoder_start:encoder_end, :]\n",
    "        Y_to_conc = Y_3d[:, encoder_start:encoder_end, :]\n",
    "        X_to_decode = X_3d[:, decoder_start:decoder_end, :]\n",
    "        Y_decoder = Y_3d[:, decoder_start:decoder_end, :]\n",
    "        \n",
    "        yield([X_to_conc,\n",
    "               X_to_decode],\n",
    "               Y_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "829/829 [==============================] - 619s 746ms/step - loss: 0.1749\n"
     ]
    }
   ],
   "source": [
    "# TRAINING -------------------------------------------------------------\n",
    "# Training on X_tr/Y_tr and validate with X_val/Y_val\n",
    "# To perform validation training on validation data should be\n",
    "# made instead of training on full data set.\n",
    "# Then validation check is made on period outside of training data\n",
    "# (included in code below).\n",
    "'''\n",
    "training_model.fit_generator(dec_enc_n_days_gen(X_tr, Y_tr, 39),\n",
    "                             validation_data = dec_enc_n_days_gen(X_val, Y_val, 39),\n",
    "                             steps_per_epoch = X_lstm.shape[0],\n",
    "                             validation_steps = X_val.shape[0],\n",
    "                             verbose = 1,\n",
    "                             epochs = 1)\n",
    "'''\n",
    "\n",
    "# Training on full dataset\n",
    "history = training_model.fit_generator(dec_enc_n_days_gen(X_lstm[:,:,:], Y_lstm[:,:,:], 39),\n",
    "                            steps_per_epoch = X_lstm[:,:,:].shape[0],\n",
    "                            verbose = 1,\n",
    "                            epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss'])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7c7be58f33a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# summarize history for accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION FUNCTION --------------------------------------------------\n",
    "\n",
    "# function takes 39 days before first prediction day (input_seq)\n",
    "# then using encoder to identify hidden states for these 39 days.\n",
    "# Next, decoder takes hidden states provided by encoder\n",
    "# and predicts number of visitors from day 2 to day 40.\n",
    "# Day 40 is the first day of target_seq.\n",
    "\n",
    "# Predicted value for day 40 is appended to features of day 41.\n",
    "# Then function takes period from day 2 to day 40 and repeat the process\n",
    "# unil all days in target sequence get their predictions. \n",
    "\n",
    "# The output of the function is the vector with predictions that has\n",
    "# following shape: 820 restaurants * 39 days * 1 predicted visitors amount\n",
    "\n",
    "def predict_sequence(inf_enc, inf_dec, input_seq, Y_input_seq, target_seq):\n",
    "    # state of input sequence produced by encoder\n",
    "    state = inf_enc.predict(input_seq)\n",
    "    \n",
    "    # restrict target sequence to the same shape as X_lstm_test\n",
    "    target_seq = target_seq[:,:, :X_lstm_test.shape[2]]\n",
    "    \n",
    "    # create vector that contains y for previous 7 days\n",
    "    t_minus_seq = np.concatenate((Y_input_seq[:,-1:,:], input_seq[:,-1:, X_lstm_test.shape[2]:-1]), axis = 2)\n",
    "    \n",
    "    # current sequence that is going to be modified each iteration of the prediction loop\n",
    "    current_seq = input_seq.copy()\n",
    "    \n",
    "    \n",
    "    # predicting outputs\n",
    "    output = np.ones([target_seq.shape[0],1,1])\n",
    "    for i in range(target_seq.shape[1]):\n",
    "        # add visitors for previous 7 days into features of a new day\n",
    "        new_day_features = np.concatenate((target_seq[:,i:i+1,:], t_minus_seq[...]), axis = 2)\n",
    "        \n",
    "        # move prediction window one day forward\n",
    "        current_seq = np.concatenate((current_seq[:,1:,:], new_day_features[:,]), axis = 1)\n",
    "        \n",
    "        \n",
    "        # predict visitors amount\n",
    "        pred = inf_dec.predict([current_seq] + state)\n",
    "        \n",
    "        # update t_minus_seq\n",
    "        t_minus_seq = np.concatenate((pred[:,-1:,:], t_minus_seq[...]), axis = 2)\n",
    "        t_minus_seq = t_minus_seq[:,:,:-1]        \n",
    "        \n",
    "        # update predicitons list\n",
    "        output = np.concatenate((output[...], pred[:,-1:,:]), axis = 1)\n",
    "        \n",
    "        # update state\n",
    "        state = inf_enc.predict(current_seq)\n",
    "    \n",
    "    return output[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE ENCODER AND DECODER -----------------------------------------    \n",
    "# inference encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# inference decoder\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs,_,_ = decoder_lstm_2(decoder_lstm_1(decoder_inputs,\n",
    "                                                    initial_state = decoder_states_inputs))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\HPI\\MP\\venv\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION ON PERIOD OUTSIDE OF TRAINING DATA -----------------------\n",
    "# should be used for validation after \"training_model\" was trained on X_tr\n",
    "'''\n",
    "val_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    start = np.random.randint(0, int(X_val.shape[1]-X_lstm_test.shape[1]*2))\n",
    "    end = start+39\n",
    "    dec_start = end\n",
    "    dec_end = dec_start+39\n",
    "\n",
    "    predictions = predict_sequence(encoder_model,\n",
    "                                   decoder_model,\n",
    "                                   X_val[:,start:end,:],\n",
    "                                   Y_val[:,start:end,:],\n",
    "                                   X_val[:,dec_start:dec_end,:])\n",
    "    score = rmsle(Y_val[:,dec_start:dec_end,:].reshape(X_val[:,start:end,:].shape[0]*39), \n",
    "                  predictions.reshape(X_val[:,start:end,:].shape[0]*39))\n",
    "    val_list.append(score)\n",
    "\n",
    "print (np.asarray(val_list).mean())\n",
    "print (np.asarray(val_list).std())\n",
    "'''\n",
    "\n",
    "# Predicting test values\n",
    "enc_dec_pred = predict_sequence(encoder_model,\n",
    "                                decoder_model,\n",
    "                                X_lstm[:,-X_lstm_test.shape[1]:,:],\n",
    "                                Y_lstm[:,-X_lstm_test.shape[1]:,:],\n",
    "                                X_lstm_test[:,:,:])\n",
    "\n",
    "# Add predicted test values to submission dataset ---------------------\n",
    "\n",
    "# Note: it is important to preserve the order of time series.\n",
    "# Thus, test set will contain all 829 lines in the same order as train set.\n",
    "# To make this 'air_store_id' is taken as in X and not in X_test (second line of 'test' variable below).\n",
    "# Only relevant results will be merged for submission dataframe\n",
    "test = df_to_reshape.loc[df_to_reshape['visit_date'].isin(X_test['visit_date'].values) &\n",
    "                         df_to_reshape['air_store_id'].isin(X['air_store_id'].values),]\n",
    "\n",
    "\n",
    "# reshape predicted values to initial shape\n",
    "test_pred = enc_dec_pred.reshape(test.shape[0], 1)\n",
    "test_pred_exp = np.exp(test_pred) - 1.0\n",
    "test_pred_exp[test_pred_exp<0] = 0\n",
    "\n",
    "# add predictions to dataframe with 'air_store_id' and 'visit_date'\n",
    "test_df_pred = test[['air_store_id', 'visit_date']].copy()\n",
    "test_df_pred['predicted'] = test_pred_exp\n",
    "\n",
    "# reverse transform of 'air_store_id'\n",
    "test_df_pred['air_store_id'] = le_id.inverse_transform(test_df_pred['air_store_id'])\n",
    "\n",
    "# finalizing submission csv file\n",
    "submission_df = submission_lstm.merge(test_df_pred,\n",
    "                                     how = 'left',\n",
    "                                     left_on = ['air_store_id', 'visit_date'],\n",
    "                                     right_on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "submission_df['visitors'] = submission_df['predicted']\n",
    "submission_df = submission_df.drop(['air_store_id', 'visit_date', 'predicted'], axis = 1)\n",
    "submission_df.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
