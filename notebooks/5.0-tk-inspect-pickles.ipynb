{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickles(pickles_dir):\n",
    "    for filename in os.listdir(pickles_dir):\n",
    "        if not 'pkl' in filename:\n",
    "            print('IGNORING', os.path.join(pickles_dir, filename))\n",
    "            continue\n",
    "        with open(os.path.join(pickles_dir, filename), 'rb') as f:\n",
    "            save_dict = pickle.load(f)\n",
    "        yield save_dict, filename\n",
    "\n",
    "def write_pickle(save_dict, pickle_path):\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "def compare(arr1, arr2):\n",
    "    left = list(np.copy(list(arr1)))\n",
    "    for elem in arr2:\n",
    "        if elem not in left:\n",
    "            print(f'B contains {elem} while A does not')\n",
    "        else:\n",
    "            left.remove(elem)\n",
    "    for elem in left:\n",
    "        print(f'A contains {elem} while B does not')\n",
    "\n",
    "def load_and_check(path, merge_datasets=False):\n",
    "    print(path)\n",
    "    global datasets, detectors\n",
    "    save_dicts = []\n",
    "    for save_dict, filename in load_pickles(path):\n",
    "        if not merge_datasets:\n",
    "            # Datasets arrays need to be equal\n",
    "            assert not datasets or np.array_equal(datasets, set(save_dict['datasets'])), compare(datasets, save_dict['datasets'])\n",
    "            if not datasets:\n",
    "                datasets = set(save_dict['datasets'])\n",
    "                print(f'Datasets:', *sorted(datasets), sep='\\n')\n",
    "        elif not all([ds in datasets for ds in save_dict['datasets']]):\n",
    "            print(f'Add datasets:', *sorted(set(save_dict[\"datasets\"]) - datasets), sep='\\n')\n",
    "            datasets = set(list(datasets) + save_dict['datasets'])\n",
    "            \n",
    "        if not all([det in detectors for det in save_dict['detectors']]):\n",
    "            print('Add detectors: ', set(save_dict['detectors']) - detectors)\n",
    "            detectors = set(list(detectors) + save_dict['detectors'])\n",
    "        save_dict['_filename'] = os.path.join(filename)\n",
    "        save_dicts.append(save_dict)\n",
    "    return save_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\reports/experiment_pollution/variance_1/allall_0.2\n",
      "Datasets:\n",
      "Syn Variance Outliers (pol=0.0, anom=0.05)\n",
      "Syn Variance Outliers (pol=0.0, anom=0.1)\n",
      "Syn Variance Outliers (pol=0.0, anom=0.2)\n",
      "Syn Variance Outliers (pol=0.0, anom=0.4)\n",
      "Syn Variance Outliers (pol=0.0, anom=0.8)\n",
      "Syn Variance Outliers (pol=0.25, anom=0.05)\n",
      "Syn Variance Outliers (pol=0.25, anom=0.1)\n",
      "Syn Variance Outliers (pol=0.25, anom=0.2)\n",
      "Syn Variance Outliers (pol=0.25, anom=0.4)\n",
      "Syn Variance Outliers (pol=0.25, anom=0.8)\n",
      "Syn Variance Outliers (pol=0.5, anom=0.05)\n",
      "Syn Variance Outliers (pol=0.5, anom=0.1)\n",
      "Syn Variance Outliers (pol=0.5, anom=0.2)\n",
      "Syn Variance Outliers (pol=0.5, anom=0.4)\n",
      "Syn Variance Outliers (pol=0.5, anom=0.8)\n",
      "Syn Variance Outliers (pol=0.75, anom=0.05)\n",
      "Syn Variance Outliers (pol=0.75, anom=0.1)\n",
      "Syn Variance Outliers (pol=0.75, anom=0.2)\n",
      "Syn Variance Outliers (pol=0.75, anom=0.4)\n",
      "Syn Variance Outliers (pol=0.75, anom=0.8)\n",
      "Syn Variance Outliers (pol=1.0, anom=0.05)\n",
      "Syn Variance Outliers (pol=1.0, anom=0.1)\n",
      "Syn Variance Outliers (pol=1.0, anom=0.2)\n",
      "Syn Variance Outliers (pol=1.0, anom=0.4)\n",
      "Syn Variance Outliers (pol=1.0, anom=0.8)\n",
      "Add detectors:  {'DAGMM-NW', 'LSTM-AD', 'AutoEncoder', 'DAGMM-NN', 'DAGMM-LW', 'LSTMED', 'Recurrent EBM', 'Donut'}\n",
      "..\\reports/experiment_pollution/variance_1/all_other\n",
      "..\\reports/experiment_pollution/variance_1/lstmad_other\n",
      "..\\reports/experiment_pollution/variance_1/lstmed_other\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n",
      "benchmarks shape: (140, 8)\n"
     ]
    }
   ],
   "source": [
    "datasets = set()\n",
    "detectors = set()\n",
    "\n",
    "outlier_type = 'extreme_1'\n",
    "\n",
    "path_1 = os.path.join('..', f'reports/experiment_pollution/{outlier_type}/rest')\n",
    "save_dicts_1 = load_and_check(path_1)\n",
    "\n",
    "path_2 = os.path.join('..', f'reports/experiment_pollution/{outlier_type}/lstmad')\n",
    "save_dicts_2 = load_and_check(path_2, True)\n",
    "\n",
    "path_3 = os.path.join('..', f'reports/experiment_pollution/{outlier_type}/lstmed')\n",
    "save_dicts_3 = load_and_check(path_3, True)\n",
    "\n",
    "# --- Merge results --- #\n",
    "\n",
    "path = os.path.join('..', 'reports', 'experiment_pollution', 'extreme_1', 'evaluators')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for dict1, dict2, dict3 in zip(save_dicts_1, save_dicts_2, save_dicts_3):\n",
    "    # We don't need the results values so drop them\n",
    "    dict1['results'] = None\n",
    "    dict1['seed'] = None\n",
    "    \n",
    "    dict1['datasets'] = [x for x in dict1['datasets'] if 'anom=0.8' not in x]\n",
    "    datasets = [x for x in datasets if 'anom=0.8' not in x]\n",
    "    \n",
    "    # dict1: drop all results except for anom=0.2\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'][dict1['benchmark_results'].dataset.str.contains('anom=0.2')]\n",
    "    \n",
    "    # dict2: contains 0.05, 0.1, 0.4 without lstmad and lstmed\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict2['benchmark_results'], ignore_index=True)\n",
    "    \n",
    "    # dict3: contains 0.05, 0.1, 0.4 for lstmad\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict3['benchmark_results'], ignore_index=True)\n",
    "    \n",
    "    if 'DAGMM-NN' in detectors:\n",
    "        detectors.remove('DAGMM-NN')\n",
    "    dict1['detectors'].remove('DAGMM-NN')\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'][dict1['benchmark_results'].algorithm != 'DAGMM-NN']\n",
    "    \n",
    "    file_path = os.path.join(path, dict1['_filename'])\n",
    "    dict1['_filename'] = None\n",
    "    \n",
    "    # Sanity check: For each ds and det there should be one entry\n",
    "    print('benchmarks shape:', dict1['benchmark_results'].shape)\n",
    "    for det, ds in itertools.product(detectors, datasets):\n",
    "        filtered = dict1['benchmark_results']\n",
    "        filtered = filtered[filtered.dataset == ds]\n",
    "        filtered = filtered[filtered.algorithm == det]\n",
    "        assert len(filtered) == 1, f'Length of results is {len(filtered)} for {det} and {ds}'\n",
    "\n",
    "    write_pickle(dict1, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 0.2 and other anom percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\reports/experiment_pollution/extreme_1/lstmed_old_ds/0.2\n",
      "Datasets:\n",
      "Syn Extreme Outliers (pol=0.0, anom=0.2)\n",
      "Syn Extreme Outliers (pol=0.25, anom=0.2)\n",
      "Syn Extreme Outliers (pol=0.5, anom=0.2)\n",
      "Syn Extreme Outliers (pol=0.75, anom=0.2)\n",
      "Syn Extreme Outliers (pol=1.0, anom=0.2)\n",
      "Add detectors:  {'LSTMED'}\n",
      "..\\reports/experiment_pollution/extreme_1/lstmed_old_ds/other\n",
      "Add datasets:\n",
      "Syn Extreme Outliers (pol=0.0, anom=0.05)\n",
      "Syn Extreme Outliers (pol=0.0, anom=0.1)\n",
      "Syn Extreme Outliers (pol=0.0, anom=0.4)\n",
      "Syn Extreme Outliers (pol=0.25, anom=0.05)\n",
      "Syn Extreme Outliers (pol=0.25, anom=0.1)\n",
      "Syn Extreme Outliers (pol=0.25, anom=0.4)\n",
      "Syn Extreme Outliers (pol=0.5, anom=0.05)\n",
      "Syn Extreme Outliers (pol=0.5, anom=0.1)\n",
      "Syn Extreme Outliers (pol=0.5, anom=0.4)\n",
      "Syn Extreme Outliers (pol=0.75, anom=0.05)\n",
      "Syn Extreme Outliers (pol=0.75, anom=0.1)\n",
      "Syn Extreme Outliers (pol=0.75, anom=0.4)\n",
      "Syn Extreme Outliers (pol=1.0, anom=0.05)\n",
      "Syn Extreme Outliers (pol=1.0, anom=0.1)\n",
      "Syn Extreme Outliers (pol=1.0, anom=0.4)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n"
     ]
    }
   ],
   "source": [
    "datasets = set()\n",
    "detectors = set()\n",
    "\n",
    "algorithm = 'lstmed'  # 'lstmad', 'rest'\n",
    "\n",
    "path_1 = os.path.join('..', f'reports/experiment_pollution/extreme_1/{algorithm}/0.2')\n",
    "save_dicts_1 = load_and_check(path_1)\n",
    "\n",
    "path_2 = os.path.join('..', f'reports/experiment_pollution/extreme_1/{algorithm}/other')\n",
    "save_dicts_2 = load_and_check(path_2, True)\n",
    "\n",
    "# --- Merge results --- #\n",
    "\n",
    "path = os.path.join('..', 'reports', 'experiment_pollution', 'extreme_1', algorithm)\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for dict1, dict2 in zip(save_dicts_1, save_dicts_2):\n",
    "    # We don't need the results values so drop them\n",
    "    dict1['results'] = None\n",
    "    dict1['seed'] = None\n",
    "    \n",
    "    # Add other levels of anom\n",
    "    dict1['datasets'] += dict2['datasets']\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict2['benchmark_results'], ignore_index=True)\n",
    "    \n",
    "    file_path = os.path.join(path, dict1['_filename'])\n",
    "    dict1['_filename'] = None\n",
    "    \n",
    "    # Sanity check: For each ds and det there should be one entry\n",
    "    print('benchmarks shape:', dict1['benchmark_results'].shape)\n",
    "    for det, ds in itertools.product(dict1['detectors'], dict1['datasets']):\n",
    "        filtered = dict1['benchmark_results']\n",
    "        filtered = filtered[filtered.dataset == ds]\n",
    "        filtered = filtered[filtered.algorithm == det]\n",
    "        assert len(filtered) == 1, f'Length of results is {len(filtered)} for {det} and {ds}'\n",
    "\n",
    "    write_pickle(dict1, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge three folders, replace LSTMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = None\n",
    "detectors = []\n",
    "\n",
    "path_1 = os.path.join('..', 'reports', 'experiment_pollution', 'trend_1', 'evaluators_old')\n",
    "save_dicts_1 = load_and_check(path_1)\n",
    "\n",
    "path_2 = os.path.join('..', 'reports', 'experiment_pollution', 'trend_1', 'additional_evaluators_lstmad')\n",
    "save_dicts_2 = load_and_check(path_2)\n",
    "\n",
    "path_3 = os.path.join('..', 'reports', 'experiment_pollution', 'trend_1', 'additional_evaluators_lstmad')\n",
    "save_dicts_3 = load_and_check(path_3)\n",
    "\n",
    "\n",
    "# --- Merge results --- #\n",
    "\n",
    "path = os.path.join('..', 'reports', 'experiment_pollution', 'trend_1', 'evaluators')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for dict1, dict2, dict3 in zip(save_dicts_1, save_dicts_2, save_dicts_3):\n",
    "    # We don't need the results values so drop them\n",
    "    dict1['results'] = None\n",
    "    dict1['seed'] = None\n",
    "    \n",
    "    # Drop results of old algorithm\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'][dict1['benchmark_results'].algorithm != 'LSTMED']\n",
    "    \n",
    "    dict1['detectors'].append('AutoEncoder')\n",
    "    dict1['detectors'].append('LSTM-AD')\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict2['benchmark_results'], ignore_index=True)\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict3['benchmark_results'], ignore_index=True)\n",
    "    \n",
    "    file_path = os.path.join(path, dict1['_filename'])\n",
    "    dict1['_filename'] = None\n",
    "    write_pickle(dict1, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36 (MP)",
   "language": "python",
   "name": "venv_mp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
