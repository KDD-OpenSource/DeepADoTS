{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickles(pickles_dir):\n",
    "    for filename in os.listdir(pickles_dir):\n",
    "        if not 'pkl' in filename:\n",
    "            print('IGNORING', os.path.join(pickles_dir, filename))\n",
    "            continue\n",
    "        with open(os.path.join(pickles_dir, filename), 'rb') as f:\n",
    "            save_dict = pickle.load(f)\n",
    "        yield save_dict, filename\n",
    "\n",
    "def write_pickle(save_dict, pickle_path):\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "def compare(arr1, arr2):\n",
    "    left = list(np.copy(list(arr1)))\n",
    "    for elem in arr2:\n",
    "        if elem not in left:\n",
    "            print(f'B contains {elem} while A does not')\n",
    "        else:\n",
    "            left.remove(elem)\n",
    "    for elem in left:\n",
    "        print(f'A contains {elem} while B does not')\n",
    "\n",
    "def load_and_check(path, merge_datasets=False):\n",
    "    print(path)\n",
    "    global datasets, detectors\n",
    "    save_dicts = []\n",
    "    for save_dict, filename in load_pickles(path):\n",
    "        if not merge_datasets:\n",
    "            # Datasets arrays need to be equal\n",
    "            assert not datasets or np.array_equal(datasets, set(save_dict['datasets'])), compare(datasets, save_dict['datasets'])\n",
    "            if not datasets:\n",
    "                datasets = set(save_dict['datasets'])\n",
    "                print(f'Datasets:', *sorted(datasets), sep='\\n')\n",
    "        elif not all([ds in datasets for ds in save_dict['datasets']]):\n",
    "            print(f'Add datasets:', *sorted(set(save_dict[\"datasets\"]) - datasets), sep='\\n')\n",
    "            datasets = set(list(datasets) + save_dict['datasets'])\n",
    "            \n",
    "        if not all([det in detectors for det in save_dict['detectors']]):\n",
    "            print('Add detectors: ', set(save_dict['detectors']) - detectors)\n",
    "            detectors = set(list(detectors) + save_dict['detectors'])\n",
    "        save_dict['_filename'] = os.path.join(filename)\n",
    "        save_dicts.append(save_dict)\n",
    "    return save_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\reports/experiment_pollution/variance_1/rest/0.01, 0.5\n",
      "Datasets:\n",
      "Syn Variance Outliers (pol=0.01, anom=0.2)\n",
      "Syn Variance Outliers (pol=0.5, anom=0.2)\n",
      "Add detectors:  {'DAGMM-NW', 'AutoEncoder', 'DAGMM-LW', 'Recurrent EBM', 'Donut'}\n",
      "..\\reports/experiment_pollution/variance_1/alle_algorithmen_0.05, 0.1, 0.2\n",
      "Add datasets:\n",
      "Syn Variance Outliers (pol=0.05, anom=0.2)\n",
      "Syn Variance Outliers (pol=0.1, anom=0.2)\n",
      "Syn Variance Outliers (pol=0.2, anom=0.2)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n",
      "benchmarks shape: (25, 8)\n"
     ]
    }
   ],
   "source": [
    "datasets = set()\n",
    "detectors = set()\n",
    "\n",
    "outlier_type = 'shift_1'\n",
    "\n",
    "path_1 = os.path.join('..', f'reports/experiment_pollution/{outlier_type}/rest/0.01, 0.5')\n",
    "save_dicts_1 = load_and_check(path_1)\n",
    "\n",
    "path_2 = os.path.join('..', f'reports/experiment_pollution/{outlier_type}/LSTM-AD/0.01, 0.5')\n",
    "save_dicts_2 = load_and_check(path_2, True)\n",
    "\n",
    "path_3 = os.path.join('..', f'reports/experiment_pollution/{outlier_type}/LSTM-ED/0.01, 0.5')\n",
    "save_dicts_3 = load_and_check(path_3, True)\n",
    "\n",
    "path_4 = os.path.join('..', f'reports/experiment_pollution/{outlier_type}/alle_algorithmen_0.05, 0.1, 0.2')\n",
    "save_dicts_4 = load_and_check(path_4, True)\n",
    "\n",
    "# --- Merge results --- #\n",
    "\n",
    "path = os.path.join('..', 'reports', 'experiment_pollution', outlier_type, 'evaluators')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for dict1, dict2, dict3, dict4 in zip(save_dicts_1, save_dicts_2, save_dicts_3, save_dicts_4):\n",
    "    # We don't need the results values so drop them\n",
    "    dict1['results'] = None\n",
    "    dict1['seed'] = None\n",
    "    \n",
    "    dict1['datasets'] = datasets\n",
    "    dict1['detectors'] = detectors\n",
    "    # dict1['datasets'] = [x for x in dict1['datasets'] if 'anom=0.8' not in x]\n",
    "    # datasets = [x for x in datasets if 'anom=0.8' not in x]\n",
    "    \n",
    "    # dict1: drop all results except for anom=0.2\n",
    "    # dict1['benchmark_results'] = dict1['benchmark_results'][dict1['benchmark_results'].dataset.str.contains('anom=0.2')]\n",
    "    \n",
    "    # dict2: contains 0.05, 0.1, 0.4 without lstmad and lstmed\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict2['benchmark_results'], ignore_index=True)\n",
    "    \n",
    "    # dict3: contains 0.05, 0.1, 0.4 for lstmad\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict3['benchmark_results'], ignore_index=True)\n",
    "    \n",
    "    # dict4: contains 0.05, 0.1, 0.4 for lstmad\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict4['benchmark_results'], ignore_index=True)\n",
    "    \n",
    "    if 'DAGMM-NN' in detectors:\n",
    "        detectors.remove('DAGMM-NN')\n",
    "        if 'DAGMM-NN' in dict1['detectors']:\n",
    "            dict1['detectors'].remove('DAGMM-NN')\n",
    "            dict1['benchmark_results'] = dict1['benchmark_results'][dict1['benchmark_results'].algorithm != 'DAGMM-NN']\n",
    "    \n",
    "    file_path = os.path.join(path, dict1['_filename'])\n",
    "    dict1['_filename'] = None\n",
    "    \n",
    "    # Sanity check: For each ds and det there should be one entry\n",
    "    print('benchmarks shape:', dict1['benchmark_results'].shape)\n",
    "    for det, ds in itertools.product(detectors, datasets):\n",
    "        filtered = dict1['benchmark_results']\n",
    "        filtered = filtered[filtered.dataset == ds]\n",
    "        filtered = filtered[filtered.algorithm == det]\n",
    "        assert len(filtered) == 1, f'Length of results is {len(filtered)} for {det} and {ds}'\n",
    "\n",
    "#     write_pickle(dict1, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F0.1-score</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>auroc</th>\n",
       "      <th>dataset</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.161342</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>AutoEncoder</td>\n",
       "      <td>0.453915</td>\n",
       "      <td>Syn Variance Outliers (pol=0.05, anom=0.2)</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.383126</td>\n",
       "      <td>0.483461</td>\n",
       "      <td>0.774444</td>\n",
       "      <td>DAGMM-NW</td>\n",
       "      <td>0.765542</td>\n",
       "      <td>Syn Variance Outliers (pol=0.05, anom=0.2)</td>\n",
       "      <td>0.381526</td>\n",
       "      <td>0.659722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.187454</td>\n",
       "      <td>0.308422</td>\n",
       "      <td>0.352222</td>\n",
       "      <td>DAGMM-LW</td>\n",
       "      <td>0.494553</td>\n",
       "      <td>Syn Variance Outliers (pol=0.05, anom=0.2)</td>\n",
       "      <td>0.185980</td>\n",
       "      <td>0.902778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.161342</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>Recurrent EBM</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>Syn Variance Outliers (pol=0.05, anom=0.2)</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.182708</td>\n",
       "      <td>0.305857</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>Donut</td>\n",
       "      <td>0.550825</td>\n",
       "      <td>Syn Variance Outliers (pol=0.05, anom=0.2)</td>\n",
       "      <td>0.181234</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.352968</td>\n",
       "      <td>0.405882</td>\n",
       "      <td>0.775556</td>\n",
       "      <td>AutoEncoder</td>\n",
       "      <td>0.670745</td>\n",
       "      <td>Syn Variance Outliers (pol=0.1, anom=0.2)</td>\n",
       "      <td>0.352041</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.275688</td>\n",
       "      <td>0.356098</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>DAGMM-NW</td>\n",
       "      <td>0.640506</td>\n",
       "      <td>Syn Variance Outliers (pol=0.1, anom=0.2)</td>\n",
       "      <td>0.274436</td>\n",
       "      <td>0.506944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.283377</td>\n",
       "      <td>0.369305</td>\n",
       "      <td>0.707778</td>\n",
       "      <td>DAGMM-LW</td>\n",
       "      <td>0.661284</td>\n",
       "      <td>Syn Variance Outliers (pol=0.1, anom=0.2)</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.534722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.394943</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.804444</td>\n",
       "      <td>Recurrent EBM</td>\n",
       "      <td>0.670736</td>\n",
       "      <td>Syn Variance Outliers (pol=0.1, anom=0.2)</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.619210</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.871111</td>\n",
       "      <td>Donut</td>\n",
       "      <td>0.762125</td>\n",
       "      <td>Syn Variance Outliers (pol=0.1, anom=0.2)</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.673991</td>\n",
       "      <td>0.401961</td>\n",
       "      <td>0.864444</td>\n",
       "      <td>AutoEncoder</td>\n",
       "      <td>0.625423</td>\n",
       "      <td>Syn Variance Outliers (pol=0.2, anom=0.2)</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.284722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.841277</td>\n",
       "      <td>0.628821</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>DAGMM-NW</td>\n",
       "      <td>0.760159</td>\n",
       "      <td>Syn Variance Outliers (pol=0.2, anom=0.2)</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.360617</td>\n",
       "      <td>0.364261</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>DAGMM-LW</td>\n",
       "      <td>0.653320</td>\n",
       "      <td>Syn Variance Outliers (pol=0.2, anom=0.2)</td>\n",
       "      <td>0.360544</td>\n",
       "      <td>0.368056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.723549</td>\n",
       "      <td>0.395939</td>\n",
       "      <td>0.867778</td>\n",
       "      <td>Recurrent EBM</td>\n",
       "      <td>0.609843</td>\n",
       "      <td>Syn Variance Outliers (pol=0.2, anom=0.2)</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.270833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.233201</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>0.587778</td>\n",
       "      <td>Donut</td>\n",
       "      <td>0.685360</td>\n",
       "      <td>Syn Variance Outliers (pol=0.2, anom=0.2)</td>\n",
       "      <td>0.231678</td>\n",
       "      <td>0.680556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    F0.1-score  F1-score  accuracy      algorithm     auroc  \\\n",
       "0     0.161342  0.275862  0.160000    AutoEncoder  0.453915   \n",
       "1     0.383126  0.483461  0.774444       DAGMM-NW  0.765542   \n",
       "2     0.187454  0.308422  0.352222       DAGMM-LW  0.494553   \n",
       "3     0.161342  0.275862  0.160000  Recurrent EBM  0.520200   \n",
       "4     0.182708  0.305857  0.288889          Donut  0.550825   \n",
       "5     0.352968  0.405882  0.775556    AutoEncoder  0.670745   \n",
       "6     0.275688  0.356098  0.706667       DAGMM-NW  0.640506   \n",
       "7     0.283377  0.369305  0.707778       DAGMM-LW  0.661284   \n",
       "8     0.394943  0.405405  0.804444  Recurrent EBM  0.670736   \n",
       "9     0.619210  0.553846  0.871111          Donut  0.762125   \n",
       "10    0.673991  0.401961  0.864444    AutoEncoder  0.625423   \n",
       "11    0.841277  0.628821  0.905556       DAGMM-NW  0.760159   \n",
       "12    0.360617  0.364261  0.794444       DAGMM-LW  0.653320   \n",
       "13    0.723549  0.395939  0.867778  Recurrent EBM  0.609843   \n",
       "14    0.233201  0.345679  0.587778          Donut  0.685360   \n",
       "\n",
       "                                       dataset  precision    recall  \n",
       "0   Syn Variance Outliers (pol=0.05, anom=0.2)   0.160000  1.000000  \n",
       "1   Syn Variance Outliers (pol=0.05, anom=0.2)   0.381526  0.659722  \n",
       "2   Syn Variance Outliers (pol=0.05, anom=0.2)   0.185980  0.902778  \n",
       "3   Syn Variance Outliers (pol=0.05, anom=0.2)   0.160000  1.000000  \n",
       "4   Syn Variance Outliers (pol=0.05, anom=0.2)   0.181234  0.979167  \n",
       "5    Syn Variance Outliers (pol=0.1, anom=0.2)   0.352041  0.479167  \n",
       "6    Syn Variance Outliers (pol=0.1, anom=0.2)   0.274436  0.506944  \n",
       "7    Syn Variance Outliers (pol=0.1, anom=0.2)   0.282051  0.534722  \n",
       "8    Syn Variance Outliers (pol=0.1, anom=0.2)   0.394737  0.416667  \n",
       "9    Syn Variance Outliers (pol=0.1, anom=0.2)   0.620690  0.500000  \n",
       "10   Syn Variance Outliers (pol=0.2, anom=0.2)   0.683333  0.284722  \n",
       "11   Syn Variance Outliers (pol=0.2, anom=0.2)   0.847059  0.500000  \n",
       "12   Syn Variance Outliers (pol=0.2, anom=0.2)   0.360544  0.368056  \n",
       "13   Syn Variance Outliers (pol=0.2, anom=0.2)   0.735849  0.270833  \n",
       "14   Syn Variance Outliers (pol=0.2, anom=0.2)   0.231678  0.680556  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict4['benchmark_results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 0.2 and other anom percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\reports/experiment_pollution/extreme_1/lstmed_old_ds/0.2\n",
      "Datasets:\n",
      "Syn Extreme Outliers (pol=0.0, anom=0.2)\n",
      "Syn Extreme Outliers (pol=0.25, anom=0.2)\n",
      "Syn Extreme Outliers (pol=0.5, anom=0.2)\n",
      "Syn Extreme Outliers (pol=0.75, anom=0.2)\n",
      "Syn Extreme Outliers (pol=1.0, anom=0.2)\n",
      "Add detectors:  {'LSTMED'}\n",
      "..\\reports/experiment_pollution/extreme_1/lstmed_old_ds/other\n",
      "Add datasets:\n",
      "Syn Extreme Outliers (pol=0.0, anom=0.05)\n",
      "Syn Extreme Outliers (pol=0.0, anom=0.1)\n",
      "Syn Extreme Outliers (pol=0.0, anom=0.4)\n",
      "Syn Extreme Outliers (pol=0.25, anom=0.05)\n",
      "Syn Extreme Outliers (pol=0.25, anom=0.1)\n",
      "Syn Extreme Outliers (pol=0.25, anom=0.4)\n",
      "Syn Extreme Outliers (pol=0.5, anom=0.05)\n",
      "Syn Extreme Outliers (pol=0.5, anom=0.1)\n",
      "Syn Extreme Outliers (pol=0.5, anom=0.4)\n",
      "Syn Extreme Outliers (pol=0.75, anom=0.05)\n",
      "Syn Extreme Outliers (pol=0.75, anom=0.1)\n",
      "Syn Extreme Outliers (pol=0.75, anom=0.4)\n",
      "Syn Extreme Outliers (pol=1.0, anom=0.05)\n",
      "Syn Extreme Outliers (pol=1.0, anom=0.1)\n",
      "Syn Extreme Outliers (pol=1.0, anom=0.4)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n",
      "benchmarks shape: (20, 8)\n"
     ]
    }
   ],
   "source": [
    "datasets = set()\n",
    "detectors = set()\n",
    "\n",
    "algorithm = 'lstmed'  # 'lstmad', 'rest'\n",
    "\n",
    "path_1 = os.path.join('..', f'reports/experiment_pollution/extreme_1/{algorithm}/0.2')\n",
    "save_dicts_1 = load_and_check(path_1)\n",
    "\n",
    "path_2 = os.path.join('..', f'reports/experiment_pollution/extreme_1/{algorithm}/other')\n",
    "save_dicts_2 = load_and_check(path_2, True)\n",
    "\n",
    "# --- Merge results --- #\n",
    "\n",
    "path = os.path.join('..', 'reports', 'experiment_pollution', 'extreme_1', algorithm)\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for dict1, dict2 in zip(save_dicts_1, save_dicts_2):\n",
    "    # We don't need the results values so drop them\n",
    "    dict1['results'] = None\n",
    "    dict1['seed'] = None\n",
    "    \n",
    "    # Add other levels of anom\n",
    "    dict1['datasets'] += dict2['datasets']\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict2['benchmark_results'], ignore_index=True)\n",
    "    \n",
    "    file_path = os.path.join(path, dict1['_filename'])\n",
    "    dict1['_filename'] = None\n",
    "    \n",
    "    # Sanity check: For each ds and det there should be one entry\n",
    "    print('benchmarks shape:', dict1['benchmark_results'].shape)\n",
    "    for det, ds in itertools.product(dict1['detectors'], dict1['datasets']):\n",
    "        filtered = dict1['benchmark_results']\n",
    "        filtered = filtered[filtered.dataset == ds]\n",
    "        filtered = filtered[filtered.algorithm == det]\n",
    "        assert len(filtered) == 1, f'Length of results is {len(filtered)} for {det} and {ds}'\n",
    "\n",
    "    write_pickle(dict1, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge three folders, replace LSTMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = None\n",
    "detectors = []\n",
    "\n",
    "path_1 = os.path.join('..', 'reports', 'experiment_pollution', 'trend_1', 'evaluators_old')\n",
    "save_dicts_1 = load_and_check(path_1)\n",
    "\n",
    "path_2 = os.path.join('..', 'reports', 'experiment_pollution', 'trend_1', 'additional_evaluators_lstmad')\n",
    "save_dicts_2 = load_and_check(path_2)\n",
    "\n",
    "path_3 = os.path.join('..', 'reports', 'experiment_pollution', 'trend_1', 'additional_evaluators_lstmad')\n",
    "save_dicts_3 = load_and_check(path_3)\n",
    "\n",
    "\n",
    "# --- Merge results --- #\n",
    "\n",
    "path = os.path.join('..', 'reports', 'experiment_pollution', 'trend_1', 'evaluators')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for dict1, dict2, dict3 in zip(save_dicts_1, save_dicts_2, save_dicts_3):\n",
    "    # We don't need the results values so drop them\n",
    "    dict1['results'] = None\n",
    "    dict1['seed'] = None\n",
    "    \n",
    "    # Drop results of old algorithm\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'][dict1['benchmark_results'].algorithm != 'LSTMED']\n",
    "    \n",
    "    dict1['detectors'].append('AutoEncoder')\n",
    "    dict1['detectors'].append('LSTM-AD')\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict2['benchmark_results'], ignore_index=True)\n",
    "    dict1['benchmark_results'] = dict1['benchmark_results'].append(dict3['benchmark_results'], ignore_index=True)\n",
    "    \n",
    "    file_path = os.path.join(path, dict1['_filename'])\n",
    "    dict1['_filename'] = None\n",
    "    write_pickle(dict1, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36 (MP)",
   "language": "python",
   "name": "venv_mp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
