{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\python\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "% matplotlib inline\n",
    "\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.evaluation.evaluator import Evaluator\n",
    "from src.datasets import SyntheticDataGenerator, MultivariateAnomalyFunction\n",
    "import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- XOR (Anomaly -> gleichzeitig)\n",
    "- Ableitung (Geschwindigkeit vs Beschleunigung)\n",
    "- Invers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot missing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [SyntheticDataGenerator.get(f'extreme_1_missing', 42, missing) for missing in np.linspace(0, 0.9, 5)]\n",
    "detectors = main.get_detectors()[:1] + main.get_detectors()[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seeds = np.random.randint(low=0, high=2 ** 32 - 1, size=2, dtype=\"uint32\")\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "    evaluator = Evaluator(datasets if datasets else get_pipeline_datasets(seed), detectors, seed=seed, output_dir='data')\n",
    "    evaluator.evaluate()\n",
    "    result = evaluator.benchmarks()\n",
    "    evaluator.benchmark_results = result\n",
    "    evaluator.export_results(f'missing-run-{i}-{seed}')\n",
    "    del evaluator  # Save memory\n",
    "    results = results.append(result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results = results.groupby([\"dataset\", \"algorithm\"], as_index=False).mean()\n",
    "std_results = results.groupby([\"dataset\", \"algorithm\"]).std(ddof=0)\n",
    "\n",
    "evaluator = Evaluator(datasets if datasets else get_pipeline_datasets(seed), detectors, seed=42, output_dir='data')\n",
    "evaluator.benchmark_results = avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"-\"*20} Average {\"-\"*20}')\n",
    "print(avg_results)\n",
    "print(f'{\"-\"*22} Std {\"-\"*22}')\n",
    "print(std_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = evaluator.export_results('missing-final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator2 = Evaluator(datasets, detectors, output_dir='data')\n",
    "evaluator2.import_results(path[5:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = evaluator2.plot_experiment_comparison('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read results from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle_dirs = ['data/exe2']\n",
    "datasets = [SyntheticDataGenerator.get(f'extreme_1_missing', 42, missing) for missing in np.linspace(0, 0.9, 5)]\n",
    "detectors = main.get_detectors()\n",
    "evaluators = []\n",
    "for dir_ in pickle_dirs:\n",
    "    for path in os.listdir(os.path.join(dir_, 'evaluators')):\n",
    "        ev = Evaluator(datasets, detectors, output_dir=dir_)\n",
    "        ev.import_results(path[:-4])\n",
    "        evaluators.append(ev)\n",
    "results = [ev.benchmark_results for ev in evaluators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aurocs = [x[['algorithm', 'dataset', 'auroc']] for x in results]\n",
    "aurocs_df = pd.concat(aurocs, axis=0, ignore_index=True)\n",
    "auroc_groups = aurocs_df.groupby(['algorithm', 'dataset'])\n",
    "aurocs_mean = auroc_groups.mean().reset_index()\n",
    "aurocs_std= auroc_groups.std().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "det_names = [str(x) for x in detectors[:]]\n",
    "ds_names = [str(x) for x in datasets]\n",
    "fig, axes = plt.subplots(len(det_names), figsize = (8, 5*len(det_names)))\n",
    "for ax, det in zip(axes.flat, det_names):\n",
    "    values = aurocs_df[aurocs_df['algorithm'] == det].drop(columns='algorithm')\n",
    "    ds_groups = values.groupby('dataset')\n",
    "    ax.boxplot([ds_groups.get_group(x)['auroc'].values for x in ds_names])\n",
    "    ax.set_title(f'Area under ROC for {det}')\n",
    "    ax.set_xticklabels([Evaluator.get_key_and_value(x)[1] for x in ds_names])\n",
    "    ax.set_xlabel('Missing percentage for extreme outliers dataset')\n",
    "    ax.set_ylabel('AUROC')\n",
    "    ax.set_ylim((0, 1))\n",
    "fig.tight_layout()\n",
    "fig.savefig('boxplots-auroc-datasets.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_ev = Evaluator(datasets, detectors, output_dir='data')\n",
    "mean_ev.set_benchmark_results(aurocs_mean)\n",
    "fig = mean_ev.plot_experiment_comparison('Missing')\n",
    "fig.savefig('missing-auroc.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate DAGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets2 = [SyntheticDataGenerator.get(f'variance_1_missing', 100, 0)]\n",
    "detectors2 = main.get_detectors()[3:4]\n",
    "ev2 = Evaluator(datasets2, detectors2, output_dir=dir_)\n",
    "ev2.evaluate()\n",
    "result = ev2.benchmarks()\n",
    "ev2.benchmark_results = result\n",
    "# evaluator.export_results(f'missing-run-{i}-{seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev2.plot_roc_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ev2.benchmark_results)\n",
    "ev2.plot_scores(9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36 (MP)",
   "language": "python",
   "name": "venv_mp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
